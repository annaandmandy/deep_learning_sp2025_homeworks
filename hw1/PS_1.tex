\documentclass[10pt]{article}
\usepackage{amsmath, amssymb}


\begin{document}

\begin{center}
    \LARGE {Problem Set 1 – Supervised Learning} \\[1em]
    \Large{DS542 – DL4DS} \\[0.5em]
    \large Spring, 2025
\end{center}

\vspace{2em}

\noindent\textbf{Note:} Refer to the equations in the \textit{Understanding Deep Learning} textbook to solve the following problems.

\vspace{5em}

\section*{Problem 2.1}

To walk “downhill” on the loss function (equation 2.5), we measure its gradient with respect to the parameters $\phi_0$ and $\phi_1$. Calculate expressions for the slopes $\frac{\partial L}{\partial \phi_0}$ and $\frac{\partial L}{\partial \phi_1}$. \\
Assume $e_i = (\phi_0 + \phi_1 x_i) - y_i$, so $L = \sum_{i=1}^{I} e_1^2$ \\
$\frac{\partial}{\partial \phi_0} = \frac{\partial}{\partial e_i}$   $\frac{\partial}{\partial \phi_1} = x_i\frac{\partial}{\partial e_i}$ \\
$\frac{\partial L}{\partial \phi_0} = \sum_{i=1}^{I} \frac{\partial}{\partial \phi_0}e_i^2 = \sum_{i=1}^{I} 2e_i = \sum_{i=1}^{I} 2(\phi_0 + \phi_1 x_i - y_i) \times 1$ \\
$\frac{\partial L}{\partial \phi_1} = \sum_{i=1}^{I} \frac{\partial}{\partial \phi_0}e_i^2 = \sum_{i=1}^{I} 2e_i = \sum_{i=1}^{I} 2(\phi_0 + \phi_1 x_i - y_i) \times x_i$ \\

\section*{Problem 2.2}

Show that we can find the minimum of the loss function in closed-form by setting the expression for the derivatives from Problem 2.1 to zero and solving for $\phi_0$ and $\phi_1$. \\
For $\phi_0 : $ \\
$\frac{\partial L}{\partial \phi_0} =  \sum_{i=1}^{I} 2(\phi_0 + \phi_1 x_i - y_i) \times 1 = 0$ \\
$I \times \phi_0 + \sum_{i=1}^{I} \phi_0 x_i - \sum_{i=1}^{I} y_i = 0$ \\
$\phi_0 = \frac{\sum_{i=1}^{I} y_i - \sum_{i=1}^{I} \phi_1 x_i}{I}$ \\
For $\phi_1 : $ \\
$\frac{\partial L}{\partial \phi_1} =  \sum_{i=1}^{I} 2(\phi_0 + \phi_1 x_i - y_i) \times x_i = 0$ \\
$\sum_{i=1}^{I} \phi_0 x_i + \sum_{i=1}^{I} \phi_1 x_i^2 - \sum_{i=1}^{I} y_i x_i = 0$ \\
$\phi_1 = \frac{\sum_{i=1}^{I} y_i x_i - \sum_{i=1}^{I} \phi_0 x_i}{\sum_{i=1}^{I} x_i^2}$ \\
\end{document}
